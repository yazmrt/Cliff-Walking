{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions\n",
    "up = 0\n",
    "right = 1\n",
    "down = 2\n",
    "left = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bu fonksiyon e-greedy algoritması ile bir alınacak aksiyonun belirlenmesini sağlıyor\n",
    "def next_action(row,col,e):\n",
    "    roll = np.random.random()\n",
    "    if roll <= e:\n",
    "        action = np.random.randint(4)\n",
    "        while (action == 0 and row == 0) or (action == 1 and col == 11) or(action == 2 and row == 3) or (action == 3 and col == 0):\n",
    "            action = np.random.randint(4)\n",
    "    else:\n",
    "        action = np.argmax(Q_table[row,col])\n",
    "        if (action == 0 and row == 0) or (action == 1 and col == 11) or(action == 2 and row == 3) or (action == 3 and col == 0):\n",
    "            order = np.argsort(Q_table[row,col])\n",
    "            for i in range(4):\n",
    "                if order[2] == i:\n",
    "                    action = i\n",
    "                    break\n",
    "            if (action == 0 and row == 0) or (action == 1 and col == 11) or(action == 2 and row == 3) or (action == 3 and col == 0):\n",
    "                for j in range(4):\n",
    "                    if order[1] == j:\n",
    "                        action = j\n",
    "                        break\n",
    "            if (action == 0 and row == 0) or (action == 1 and col == 11) or(action == 2 and row == 3) or (action == 3 and col == 0):\n",
    "                for m in range(4):\n",
    "                    if order[0] == m:\n",
    "                        action = m\n",
    "                        break                                              \n",
    "    return action\n",
    "\n",
    "#Bu fonksiyon durumu güncelliyor ve eğer cliff bölgesine geldiyse oyunu sonlandırmak için bir kontrol değeri veriyor.\n",
    "def take_action(row,col,action):\n",
    "    cliff_check = False\n",
    "    if (action == 0):\n",
    "        new_row = row - 1\n",
    "        new_col = col\n",
    "    elif (action == 1):\n",
    "        new_col = col + 1\n",
    "        new_row = row\n",
    "    elif (action == 2):\n",
    "        new_row = row + 1\n",
    "        new_col = col\n",
    "    elif (action == 3):\n",
    "        new_col = col - 1\n",
    "        new_row = row\n",
    "#uçuruma gelip gelmediğini test etmek için.        \n",
    "    if (row == 3 and (1 <= col <= 11)):\n",
    "        cliff_check = True\n",
    "    return new_row,new_col,cliff_check\n",
    "\n",
    "# Bulunan durumdaki Q değerini Q_learning için güncelliyor.\n",
    "def update_Q(row,col,rewards,Q_table,gama,lr,next_row,next_col,action):\n",
    "    Q_old = Q_table[row,col,action]\n",
    "    max_action = next_action(next_row,next_col,-1)\n",
    "    Q_new = Q_old + lr*(rewards[row,col] + gama*Q_table[next_row,next_col,max_action] - Q_old)\n",
    "    return Q_new\n",
    "\n",
    "# Bulunan durumdaki Q değerini SARSA_learning için güncelliyor.\n",
    "def update_Q_Sarsa(row,col,rewards,Q_table,gama,lr,next_row,next_col,action,e):\n",
    "    Q_old = Q_table[row,col,action]\n",
    "    new_action = next_action(next_row,next_col,e)\n",
    "    Q_new = Q_old + lr*(rewards[row,col] + gama*Q_table[next_row,next_col,new_action] - Q_old)\n",
    "    return Q_new,new_action\n",
    "\n",
    "#Grafikleri çizmek için\n",
    "def plotting(xlabel,ylabel_q,ylabel_sarsa):\n",
    "    plt.ylabel('Reward per episode')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.plot(xlabel,ylabel_q, label=\"line 1\")\n",
    "    plt.plot(xlabel,ylabel_sarsa, label= \"line 2\")\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "def animation(last_game):      \n",
    "    gridworld = np.zeros([4, 12])\n",
    "# add cliff marked as -1\n",
    "    gridworld[3, 1:11] = -100\n",
    "    for i in range(0, 4):\n",
    "        print('-------------------------------------------------')\n",
    "        out = '| '\n",
    "        for j in range(0, 12):\n",
    "            if gridworld[i, j] == -100:\n",
    "                inside = 'C'\n",
    "            if gridworld[i, j] == 0:\n",
    "                inside = ' '\n",
    "            for m in last_game:\n",
    "                if [i, j] == m:\n",
    "                    inside = 'P'\n",
    "                    break\n",
    "            if (i, j) == (3,11):\n",
    "                inside = 'F'\n",
    "            out += inside + ' | '\n",
    "        print(out)\n",
    "    print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(Q_table,rewards,episode,e,gama,learning_rate,ylabel,xlabel,total_reward):\n",
    "    mean_rewards = []\n",
    "    last_state = []\n",
    "    for i in range(episode):\n",
    "#         print(f\"Reward {total_reward}\")\n",
    "#         print(f\"Episode {i}\")\n",
    "        row = 3\n",
    "        col = 0\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            if i == episode-1:\n",
    "                last_state.append([row,col])\n",
    "            if (row == 3) and (col == 11):\n",
    "                break\n",
    "            total_reward += rewards[row,col]\n",
    "#             print(f\"Row {row}\")\n",
    "#             print(f\"Col {col} \\n\")\n",
    "            action = next_action(row,col,e)\n",
    "            next_row,next_col,cliff_check = take_action(row,col,action)\n",
    "            Q_table[row,col,action] = update_Q(row,col,rewards,Q_table,gama,learning_rate,next_row,next_col,action)\n",
    "            row = next_row\n",
    "            col = next_col\n",
    "            if cliff_check == True:\n",
    "                break\n",
    "#Tablolar ve grafikler için                \n",
    "        ylabel.append(total_reward)\n",
    "        mean_reward = np.mean(ylabel)\n",
    "        mean_rewards.append(mean_reward)\n",
    "#    print(Q_table)\n",
    "#    plotting(Q_table,xlabel,mean_rewards)\n",
    "    animation(last_state)\n",
    "    print(last_state)\n",
    "    return mean_rewards\n",
    "    \n",
    "def sarsa_learning(Q_table,rewards,episode,e,gama,learning_rate,ylabel,xlabel,total_reward):\n",
    "    mean_rewards = []\n",
    "    last_state = []\n",
    "    for i in range(episode):\n",
    "#         print(f\"Reward {total_reward}\")\n",
    "#         print(f\"Episode {i}\")\n",
    "        row = 3\n",
    "        col = 0\n",
    "        total_reward = 0\n",
    "        action = next_action(row,col,e)\n",
    "        while True:\n",
    "            if i == episode-1:\n",
    "                last_state.append([row,col])\n",
    "            if (row == 3) and (col == 11):\n",
    "                break\n",
    "            total_reward += rewards[row,col]\n",
    "#             print(f\"Row {row}\")\n",
    "#             print(f\"Col {col} \\n\")\n",
    "            next_row,next_col,cliff_check = take_action(row,col,action)\n",
    "            Q_table[row,col,action],action = update_Q_Sarsa(row,col,rewards,Q_table,gama,learning_rate,next_row,next_col,action,e)\n",
    "            row = next_row\n",
    "            col = next_col\n",
    "            if cliff_check == True:\n",
    "                break\n",
    "#Tablolar ve grafikler için \n",
    "        ylabel.append(total_reward)\n",
    "        mean_reward = np.mean(ylabel)\n",
    "        mean_rewards.append(mean_reward)\n",
    "#    print(Q_table)\n",
    "    animation(last_state)\n",
    "    return mean_rewards\n",
    "#    plotting(Q_table,xlabel,mean_rewards)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initializations for Sarsa\n",
    "Q_table = np.zeros((4,12,4))\n",
    "rewards = np.full((4,12),-1)\n",
    "episode = 550\n",
    "rewards[3,1:11] = -100\n",
    "rewards[3,11] = 0\n",
    "e = 0.1\n",
    "gama = 0.9\n",
    "learning_rate = 0.40\n",
    "xlabel = np.linspace(1,episode+1,num = episode)\n",
    "total_reward = 0\n",
    "ylabel_Sarsa = []\n",
    "\n",
    "mean_rewards_sarsa = sarsa_learning(Q_table,rewards,episode,e,gama,learning_rate,ylabel_Sarsa,xlabel,total_reward)\n",
    "\n",
    "\n",
    "#initializations for Q learning\n",
    "Q_table = np.zeros((4,12,4))\n",
    "ylabel_Q = []\n",
    "total_reward = 0\n",
    "mean_rewards_q = q_learning(Q_table,rewards,episode,e,gama,learning_rate,ylabel_Q,xlabel,total_reward)\n",
    "\n",
    "plotting(xlabel,mean_rewards_q,mean_rewards_sarsa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
